{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandidos multibrazo\n",
    "El objetivo de este ejercicio es programar las diferentes estrategias para equilibrar exploración y explotación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Algoritmo incremental epsilon greedy:\n",
    "\n",
    "<img src=\"simple_bandit_algo.PNG\">\n",
    "\n",
    "### 2. Algoritmo no estacionario con alfa constante:\n",
    "\n",
    "<img src=\"non_stationary.PNG\">\n",
    "\n",
    "### 3. Algoritmo con cota superior de confianza:\n",
    "\n",
    "<img src=\"UCB.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#pip3 install tqdm\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "\n",
    "class Bandit:\n",
    "    # @k_arm: # de brazos\n",
    "    def __init__(self, k_arm=10, true_reward=0.):\n",
    "        self.k = k_arm\n",
    "        self.true_reward = true_reward\n",
    "    \n",
    "    def reset(self):\n",
    "        # recompensa real para cada acción:\n",
    "        self.q_true = np.random.randn(self.k) + self.true_reward\n",
    "        self.best_action = np.argmax(self.q_true)\n",
    "        \n",
    "    def step(self, action):\n",
    "        # genera una recompensa bajo N(recompensa real, 1)\n",
    "        reward = np.random.randn() + self.q_true[action]\n",
    "        return reward\n",
    "    \n",
    "class SimpleBanditAlgo:\n",
    "    # @epsilon: proabilidad de exploración para el algoritmo epsilon-greedy\n",
    "    # @initial: initial estimation for each action\n",
    "    def __init__(self, initial, epsilon, k_arm=10):\n",
    "        self.epsilon = epsilon\n",
    "        self.initial = initial\n",
    "        self.k = k_arm\n",
    "        self.indices = np.arange(self.k)\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        # estimación de cada acción\n",
    "        self.q_estimation = np.zeros(self.k) + self.initial\n",
    "        # # de veces que se eligió cada acción\n",
    "        self.action_count = np.zeros(self.k)\n",
    "        \n",
    "    def act(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.indices)\n",
    "        q_best = np.max(self.q_estimation)\n",
    "        return np.random.choice([action for action, q in enumerate(self.q_estimation) if q == q_best])\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        self.action_count[action] += 1\n",
    "        ### COMPLETAR la próxima línea ###\n",
    "        # self.q_estimation[action] += \n",
    "\n",
    "    \n",
    "class ConstantStepSizeAlgo:\n",
    "    # @epsilon: proabilidad de exploración para el algoritmo epsilon-greedy\n",
    "    # @initial: initial estimation for each action\n",
    "    # @alpha: step\n",
    "    def __init__(self, initial, epsilon, alpha, k_arm=10):\n",
    "        self.epsilon = epsilon\n",
    "        self.initial = initial\n",
    "        self.alpha = alpha\n",
    "        self.k = k_arm\n",
    "        self.indices = np.arange(self.k)\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        # estimación de cada acción\n",
    "        self.q_estimation = np.zeros(self.k) + self.initial\n",
    "        # # de veces que se eligió cada acción\n",
    "        self.action_count = np.zeros(self.k)\n",
    "        \n",
    "    def act(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.indices)\n",
    "        q_best = np.max(self.q_estimation)\n",
    "        return np.random.choice([action for action, q in enumerate(self.q_estimation) if q == q_best])\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        self.action_count[action] += 1\n",
    "        ### COMPLETAR la próxima línea ###\n",
    "        # self.q_estimation[action] +=\n",
    "\n",
    "class UCBAlgo:\n",
    "    # @epsilon: proabilidad de exploración para el algoritmo epsilon-greedy\n",
    "    # @initial: initial estimation for each action\n",
    "    # @alpha: step\n",
    "    def __init__(self, initial, epsilon, alpha, UCB_param, k_arm=10):\n",
    "        self.epsilon = epsilon\n",
    "        self.initial = initial\n",
    "        self.alpha = alpha\n",
    "        self.UCB_param = UCB_param\n",
    "        self.k = k_arm\n",
    "        self.indices = np.arange(self.k)\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        # estimación de cada acción\n",
    "        self.q_estimation = np.zeros(self.k) + self.initial\n",
    "        # # de veces que se eligió cada acción\n",
    "        self.action_count = np.zeros(self.k)\n",
    "        self.time = 0\n",
    "    \n",
    "    def act(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.indices)\n",
    "       \n",
    "        ### COMPLETAR la próxima línea ###\n",
    "        #UCB_estimation = \n",
    "        q_best = np.max(UCB_estimation)\n",
    "        return np.random.choice([action for action, q in enumerate(UCB_estimation) if q == q_best])\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        self.time += 1\n",
    "        self.action_count[action] += 1\n",
    "        self.q_estimation[action] += self.alpha * (reward - self.q_estimation[action])\n",
    "\n",
    "def simulate(runs, time, bandit, algos):\n",
    "    best_action_counts = np.zeros((len(algos), runs, time))\n",
    "    rewards = np.zeros((len(algos), runs, time))\n",
    "    for i, algo in enumerate(algos):\n",
    "        for r in tqdm(range(runs)):\n",
    "            bandit.reset()\n",
    "            algo.reset()\n",
    "            for t in range(time):\n",
    "                action = algo.act()\n",
    "                reward = bandit.step(action)\n",
    "                algo.update(action, reward)\n",
    "                rewards[i, r, t] = reward\n",
    "                if action == bandit.best_action:\n",
    "                    best_action_counts[i, r, t] = 1\n",
    "    best_action_counts = best_action_counts.mean(axis=1)\n",
    "    rewards = rewards.mean(axis=1)\n",
    "    return best_action_counts, rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = [SimpleBanditAlgo(5, 0.0), \n",
    "         SimpleBanditAlgo(0, 0.1), \n",
    "         SimpleBanditAlgo(0, 0.01), \n",
    "         ConstantStepSizeAlgo(0, 0.1, 0.1), \n",
    "         UCBAlgo(0, 0.1, 0.1, 2)]\n",
    "\n",
    "bandit = Bandit()\n",
    "best_action_counts, rewards = simulate(1000, 2000, bandit, algos)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.plot(best_action_counts[0], label='epsilon = 0, q = 5')\n",
    "plt.plot(best_action_counts[1], label='epsilon = 0.1, q = 0')\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.plot(rewards[1], label='epsilon = 0.1')\n",
    "plt.plot(rewards[2], label='epsilon = 0.01')\n",
    "plt.plot(rewards[3], label='alpha = 0.1, epsilon = 0.1')\n",
    "plt.plot(rewards[4], label='alpha = 0.1, epsilon = 0.1, UCB param = 2')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
