{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid World es el ambiente del libro de Sutton del capítulo 4. Un agente está en una grilla de MxN y el objetivo es llegar al estado terminal esquina superior izquierda o esquina inferior derecha.\n",
    "\n",
    "Por ejemplo, una grilla de 4x4 se ve así:\n",
    "\n",
    "T  o  o  o <br>\n",
    "o  o  o  o <br>\n",
    "o  x  o  o <br>\n",
    "o  o  o  T\n",
    "\n",
    "x es la posición del agente. T son los estados terminales.\n",
    "\n",
    "El agente puede ir hacia arriba(0), la derecha(1), abajo(2), izquierda(3). Si se choca con las paredes se queda estático. Cada movimiento 'cuesta' una unidad de reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env._render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(2)\n",
    "env._render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este ejercicio es evaluar la política aleatoria (que se mueve en las cuatro direcciones con la misma probabilidad).\n",
    "\n",
    "Recordar las ecuaciones y el algoritmo (de Sutton capítulo 4):\n",
    "\n",
    "<img src=\"ecuacion 4.5.PNG\">\n",
    "<img src=\"algoritmo de evaluacion.PNG\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluar una política dado un ambiente y una descripción completa\n",
    "    de la dinámica del ambiente.\n",
    "    \n",
    "    Argumentos:\n",
    "        política: matriz de tamaño [S, A] representando la política.\n",
    "        env: ambiente de OpenAI representadno las probabilidades de transición\n",
    "        del ambiente. \n",
    "        env.P[s][a] es una lista de tuplas (probabilidad, próximo_estado, recompensa, done)\n",
    "        env.nS es el número de estados en el ambiente\n",
    "        env.nA es el número de acciones en el ambiente\n",
    "        theta: para la evaluación de la política una vez que la función de valor cambia menos que\n",
    "        theta para todos los estados\n",
    "        discount_factor: factor de descuento gama.\n",
    "        \n",
    "    Retorna:\n",
    "        Vector de longitud env.nS que representa la función de valor.\n",
    "    \"\"\"\n",
    "    # Empezar con función de valor nula\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        # TODO: Implementar!\n",
    "        \n",
    "        #TIP: enumerate(lista) permite iterar sobre indice, elemento\n",
    "        \n",
    "        # por cada estado en el env [0,1,...,nS-1]:\n",
    "          # inicializar en 0 la funcion valor para ese estado\n",
    "          # por cada accion posible:\n",
    "            # por cada posible transicion dado ese estado-accion:\n",
    "              # usar la formula para sumar el termino a la funcion valor del estado\n",
    "          \n",
    "          # usar una variable para guardar el cambio maximo de nueva funcion valor vs anterior funcion valor\n",
    "          # guardar funcion valor para el estado\n",
    "            \n",
    "        # si el cambio maximo en el update de la funcion valor para todos los estados es menor a theta, parar\n",
    "        break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "v = policy_eval(random_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que la evaluación de la política funcionó como esperábamos\n",
    "expected_v = np.array([0, -14, -20, -22, -14, -18, -20, -20, -20, -20, -18, -14, -22, -20, -14, 0])\n",
    "np.testing.assert_array_almost_equal(v, expected_v, decimal=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
