{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "argv cannot be an empty list, and must contain the program name as the first element.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c7e5faf7068f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m#    'Directory where Census data is stored.')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mFLAGS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_contexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, argv, known_only)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       raise ValueError(\n\u001b[0;32m--> 618\u001b[0;31m           \u001b[0;34m'argv cannot be an empty list, and must contain the program name as '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m           'the first element.')\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: argv cannot be an empty list, and must contain the program name as the first element."
     ]
    }
   ],
   "source": [
    "# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Simple example of contextual bandits simulation.\n",
    "\n",
    "Code corresponding to:\n",
    "Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks\n",
    "for Thompson Sampling, by Carlos Riquelme, George Tucker, and Jasper Snoek.\n",
    "https://arxiv.org/abs/1802.09127\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from bandits.algorithms.bootstrapped_bnn_sampling import BootstrappedBNNSampling\n",
    "from bandits.core.contextual_bandit import run_contextual_bandit\n",
    "from bandits.data.data_sampler import sample_adult_data\n",
    "from bandits.data.data_sampler import sample_census_data\n",
    "from bandits.data.data_sampler import sample_covertype_data\n",
    "from bandits.data.data_sampler import sample_jester_data\n",
    "from bandits.data.data_sampler import sample_mushroom_data\n",
    "from bandits.data.data_sampler import sample_statlog_data\n",
    "from bandits.data.data_sampler import sample_stock_data\n",
    "from bandits.algorithms.fixed_policy_sampling import FixedPolicySampling\n",
    "from bandits.algorithms.linear_full_posterior_sampling import LinearFullPosteriorSampling\n",
    "from bandits.algorithms.neural_linear_sampling import NeuralLinearPosteriorSampling\n",
    "from bandits.algorithms.parameter_noise_sampling import ParameterNoiseSampling\n",
    "from bandits.algorithms.posterior_bnn_sampling import PosteriorBNNSampling\n",
    "from bandits.data.synthetic_data_sampler import sample_linear_data\n",
    "from bandits.data.synthetic_data_sampler import sample_sparse_linear_data\n",
    "from bandits.data.synthetic_data_sampler import sample_wheel_bandit_data\n",
    "from bandits.algorithms.uniform_sampling import UniformSampling\n",
    "\n",
    "# Set up your file routes to the data files.\n",
    "base_route = os.getcwd()\n",
    "data_route = 'contextual_bandits/datasets'\n",
    "\n",
    "#mushroom_data = os.path.join(base_route, data_route, 'mushroom.data')\n",
    "#financial_data = os.path.join(base_route, data_route, 'raw_stock_contexts')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS.set_default('alsologtostderr', True)\n",
    "#flags.DEFINE_string('logdir', '/tmp/bandits/', 'Base directory to save output')\n",
    "#flags.DEFINE_string(\n",
    "#    'mushroom_data',\n",
    "#    os.path.join(base_route, data_route, 'mushroom.data'),\n",
    "#    'Directory where Mushroom data is stored.')\n",
    "#flags.DEFINE_string(\n",
    "#    'financial_data',\n",
    "#    os.path.join(base_route, data_route, 'raw_stock_contexts'),\n",
    "#    'Directory where Financial data is stored.')\n",
    "#flags.DEFINE_string(\n",
    "#    'jester_data',\n",
    "#    os.path.join(base_route, data_route, 'jester_data_40jokes_19181users.npy'),\n",
    "#    'Directory where Jester data is stored.')\n",
    "#flags.DEFINE_string(\n",
    "#    'statlog_data',\n",
    "#    os.path.join(base_route, data_route, 'shuttle.trn'),\n",
    "#    'Directory where Statlog data is stored.')\n",
    "#flags.DEFINE_string(\n",
    "#    'adult_data',\n",
    "#    os.path.join(base_route, data_route, 'adult.full'),\n",
    "#    'Directory where Adult data is stored.')\n",
    "#flags.DEFINE_string(\n",
    "#    'covertype_data',\n",
    "#    os.path.join(base_route, data_route, 'covtype.data'),\n",
    "#    'Directory where Covertype data is stored.')\n",
    "#flags.DEFINE_string(\n",
    "#    'census_data',\n",
    "#    os.path.join(base_route, data_route, 'USCensus1990.data.txt'),\n",
    "#    'Directory where Census data is stored.')\n",
    "\n",
    "FLAGS(None)\n",
    "\n",
    "def sample_data(data_type, num_contexts=None):\n",
    "    \"\"\"Sample data from given 'data_type'.\n",
    "\n",
    "    Args:\n",
    "    data_type: Dataset from which to sample.\n",
    "    num_contexts: Number of contexts to sample.\n",
    "\n",
    "    Returns:\n",
    "    dataset: Sampled matrix with rows: (context, reward_1, ..., reward_num_act).\n",
    "    opt_rewards: Vector of expected optimal reward for each context.\n",
    "    opt_actions: Vector of optimal action for each context.\n",
    "    num_actions: Number of available actions.\n",
    "    context_dim: Dimension of each context.\n",
    "    \"\"\"\n",
    "    if data_type == 'linear':\n",
    "        # Create linear dataset\n",
    "        num_actions = 8\n",
    "        context_dim = 10\n",
    "        noise_stds = [0.01 * (i + 1) for i in range(num_actions)]\n",
    "        dataset, _, opt_linear = sample_linear_data(num_contexts, context_dim,\n",
    "                                                    num_actions, sigma=noise_stds)\n",
    "        opt_rewards, opt_actions = opt_linear\n",
    "    elif data_type == 'sparse_linear':\n",
    "        # Create sparse linear dataset\n",
    "        num_actions = 7\n",
    "        context_dim = 10\n",
    "        noise_stds = [0.01 * (i + 1) for i in range(num_actions)]\n",
    "        num_nnz_dims = int(context_dim / 3.0)\n",
    "        dataset, _, opt_sparse_linear = sample_sparse_linear_data(\n",
    "            num_contexts, context_dim, num_actions, num_nnz_dims, sigma=noise_stds)\n",
    "        opt_rewards, opt_actions = opt_sparse_linear\n",
    "    elif data_type == 'mushroom':\n",
    "        # Create mushroom dataset\n",
    "        num_actions = 2\n",
    "        context_dim = 117\n",
    "        file_name = FLAGS.mushroom_data\n",
    "        #file_name = mushroom_data\n",
    "        dataset, opt_mushroom = sample_mushroom_data(file_name, num_contexts)\n",
    "        opt_rewards, opt_actions = opt_mushroom\n",
    "    elif data_type == 'financial':\n",
    "        num_actions = 8\n",
    "        context_dim = 21\n",
    "        num_contexts = min(3713, num_contexts)\n",
    "        noise_stds = [0.01 * (i + 1) for i in range(num_actions)]\n",
    "        file_name = FLAGS.financial_data\n",
    "        #file_name = financial_data\n",
    "        dataset, opt_financial = sample_stock_data(file_name, context_dim,\n",
    "                                                   num_actions, num_contexts,\n",
    "                                                   noise_stds, shuffle_rows=True)\n",
    "        opt_rewards, opt_actions = opt_financial\n",
    "    elif data_type == 'jester':\n",
    "        num_actions = 8\n",
    "        context_dim = 32\n",
    "        num_contexts = min(19181, num_contexts)\n",
    "        file_name = FLAGS.jester_data\n",
    "        dataset, opt_jester = sample_jester_data(file_name, context_dim,\n",
    "                                                 num_actions, num_contexts,\n",
    "                                                 shuffle_rows=True,\n",
    "                                                 shuffle_cols=True)\n",
    "        opt_rewards, opt_actions = opt_jester\n",
    "    elif data_type == 'statlog':\n",
    "        file_name = FLAGS.statlog_data\n",
    "        num_actions = 7\n",
    "        num_contexts = min(43500, num_contexts)\n",
    "        sampled_vals = sample_statlog_data(file_name, num_contexts,\n",
    "                                           shuffle_rows=True)\n",
    "        contexts, rewards, (opt_rewards, opt_actions) = sampled_vals\n",
    "        dataset = np.hstack((contexts, rewards))\n",
    "        context_dim = contexts.shape[1]\n",
    "    elif data_type == 'adult':\n",
    "        file_name = FLAGS.adult_data\n",
    "        num_actions = 14\n",
    "        num_contexts = min(45222, num_contexts)\n",
    "        sampled_vals = sample_adult_data(file_name, num_contexts,\n",
    "                                         shuffle_rows=True)\n",
    "        contexts, rewards, (opt_rewards, opt_actions) = sampled_vals\n",
    "        dataset = np.hstack((contexts, rewards))\n",
    "        context_dim = contexts.shape[1]\n",
    "    elif data_type == 'covertype':\n",
    "        file_name = FLAGS.covertype_data\n",
    "        num_actions = 7\n",
    "        num_contexts = min(150000, num_contexts)\n",
    "        sampled_vals = sample_covertype_data(file_name, num_contexts,\n",
    "                                             shuffle_rows=True)\n",
    "        contexts, rewards, (opt_rewards, opt_actions) = sampled_vals\n",
    "        dataset = np.hstack((contexts, rewards))\n",
    "        context_dim = contexts.shape[1]\n",
    "    elif data_type == 'census':\n",
    "        file_name = FLAGS.census_data\n",
    "        num_actions = 9\n",
    "        num_contexts = min(150000, num_contexts)\n",
    "        sampled_vals = sample_census_data(file_name, num_contexts,\n",
    "                                          shuffle_rows=True)\n",
    "        contexts, rewards, (opt_rewards, opt_actions) = sampled_vals\n",
    "        dataset = np.hstack((contexts, rewards))\n",
    "        context_dim = contexts.shape[1]\n",
    "    elif data_type == 'wheel':\n",
    "        delta = 0.95\n",
    "        num_actions = 5\n",
    "        context_dim = 2\n",
    "        mean_v = [1.0, 1.0, 1.0, 1.0, 1.2]\n",
    "        std_v = [0.05, 0.05, 0.05, 0.05, 0.05]\n",
    "        mu_large = 50\n",
    "        std_large = 0.01\n",
    "        dataset, opt_wheel = sample_wheel_bandit_data(num_contexts, delta,\n",
    "                                                      mean_v, std_v,\n",
    "                                                      mu_large, std_large)\n",
    "        opt_rewards, opt_actions = opt_wheel\n",
    "\n",
    "    return dataset, opt_rewards, opt_actions, num_actions, context_dim\n",
    "\n",
    "\n",
    "def display_results(algos, opt_rewards, opt_actions, h_rewards, t_init, name):\n",
    "    \"\"\"Displays summary statistics of the performance of each algorithm.\"\"\"\n",
    "\n",
    "    print('---------------------------------------------------')\n",
    "    print('---------------------------------------------------')\n",
    "    print('{} bandit completed after {} seconds.'.format(name, time.time() - t_init))\n",
    "    print('---------------------------------------------------')\n",
    "\n",
    "    performance_pairs = []\n",
    "    for j, a in enumerate(algos):\n",
    "        performance_pairs.append((a.name, np.sum(h_rewards[:, j])))\n",
    "        performance_pairs = sorted(performance_pairs,\n",
    "                                 key=lambda elt: elt[1],\n",
    "                                 reverse=True)\n",
    "    for i, (name, reward) in enumerate(performance_pairs):\n",
    "        print('{:3}) {:20}| \\t \\t total reward = {:10}.'.format(i, name, reward))\n",
    "\n",
    "        print('---------------------------------------------------')\n",
    "        print('Optimal total reward = {}.'.format(np.sum(opt_rewards)))\n",
    "        print('Frequency of optimal actions (action, frequency):')\n",
    "        print([[elt, list(opt_actions).count(elt)] for elt in set(opt_actions)])\n",
    "        print('---------------------------------------------------')\n",
    "        print('---------------------------------------------------')\n",
    "\n",
    "# Problem parameters\n",
    "num_contexts = 2000\n",
    "\n",
    "# Choose data source among:\n",
    "# {linear, sparse_linear, mushroom, financial, jester,\n",
    "#  statlog, adult, covertype, census, wheel}\n",
    "data_type = 'financial'\n",
    "\n",
    "# Create dataset\n",
    "sampled_vals = sample_data(data_type, num_contexts)\n",
    "dataset, opt_rewards, opt_actions, num_actions, context_dim = sampled_vals\n",
    "\n",
    "# Define hyperparameters and algorithms\n",
    "# Define hyperparameters and algorithms\n",
    "hparams = tf.contrib.training.HParams(num_actions=num_actions)\n",
    "\n",
    "hparams_linear = tf.contrib.training.HParams(num_actions=num_actions,\n",
    "                                           context_dim=context_dim,\n",
    "                                           a0=6,\n",
    "                                           b0=6,\n",
    "                                           lambda_prior=0.25,\n",
    "                                           initial_pulls=2)\n",
    "\n",
    "hparams_rms = tf.contrib.training.HParams(num_actions=num_actions,\n",
    "                                        context_dim=context_dim,\n",
    "                                        init_scale=0.3,\n",
    "                                        activation=tf.nn.relu,\n",
    "                                        layer_sizes=[50],\n",
    "                                        batch_size=512,\n",
    "                                        activate_decay=True,\n",
    "                                        initial_lr=0.1,\n",
    "                                        max_grad_norm=5.0,\n",
    "                                        show_training=False,\n",
    "                                        freq_summary=1000,\n",
    "                                        buffer_s=-1,\n",
    "                                        initial_pulls=2,\n",
    "                                        optimizer='RMS',\n",
    "                                        reset_lr=True,\n",
    "                                        lr_decay_rate=0.5,\n",
    "                                        training_freq=50,\n",
    "                                        training_epochs=100,\n",
    "                                        p=0.95,\n",
    "                                        q=3)\n",
    "\n",
    "hparams_dropout = tf.contrib.training.HParams(num_actions=num_actions,\n",
    "                                            context_dim=context_dim,\n",
    "                                            init_scale=0.3,\n",
    "                                            activation=tf.nn.relu,\n",
    "                                            layer_sizes=[50],\n",
    "                                            batch_size=512,\n",
    "                                            activate_decay=True,\n",
    "                                            initial_lr=0.1,\n",
    "                                            max_grad_norm=5.0,\n",
    "                                            show_training=False,\n",
    "                                            freq_summary=1000,\n",
    "                                            buffer_s=-1,\n",
    "                                            initial_pulls=2,\n",
    "                                            optimizer='RMS',\n",
    "                                            reset_lr=True,\n",
    "                                            lr_decay_rate=0.5,\n",
    "                                            training_freq=50,\n",
    "                                            training_epochs=100,\n",
    "                                            use_dropout=True,\n",
    "                                            keep_prob=0.80)\n",
    "\n",
    "hparams_bbb = tf.contrib.training.HParams(num_actions=num_actions,\n",
    "                                        context_dim=context_dim,\n",
    "                                        init_scale=0.3,\n",
    "                                        activation=tf.nn.relu,\n",
    "                                        layer_sizes=[50],\n",
    "                                        batch_size=512,\n",
    "                                        activate_decay=True,\n",
    "                                        initial_lr=0.1,\n",
    "                                        max_grad_norm=5.0,\n",
    "                                        show_training=False,\n",
    "                                        freq_summary=1000,\n",
    "                                        buffer_s=-1,\n",
    "                                        initial_pulls=2,\n",
    "                                        optimizer='RMS',\n",
    "                                        use_sigma_exp_transform=True,\n",
    "                                        cleared_times_trained=10,\n",
    "                                        initial_training_steps=100,\n",
    "                                        noise_sigma=0.1,\n",
    "                                        reset_lr=False,\n",
    "                                        training_freq=50,\n",
    "                                        training_epochs=100)\n",
    "\n",
    "hparams_nlinear = tf.contrib.training.HParams(num_actions=num_actions,\n",
    "                                            context_dim=context_dim,\n",
    "                                            init_scale=0.3,\n",
    "                                            activation=tf.nn.relu,\n",
    "                                            layer_sizes=[50],\n",
    "                                            batch_size=512,\n",
    "                                            activate_decay=True,\n",
    "                                            initial_lr=0.1,\n",
    "                                            max_grad_norm=5.0,\n",
    "                                            show_training=False,\n",
    "                                            freq_summary=1000,\n",
    "                                            buffer_s=-1,\n",
    "                                            initial_pulls=2,\n",
    "                                            reset_lr=True,\n",
    "                                            lr_decay_rate=0.5,\n",
    "                                            training_freq=1,\n",
    "                                            training_freq_network=50,\n",
    "                                            training_epochs=100,\n",
    "                                            a0=6,\n",
    "                                            b0=6,\n",
    "                                            lambda_prior=0.25)\n",
    "\n",
    "hparams_nlinear2 = tf.contrib.training.HParams(num_actions=num_actions,\n",
    "                                             context_dim=context_dim,\n",
    "                                             init_scale=0.3,\n",
    "                                             activation=tf.nn.relu,\n",
    "                                             layer_sizes=[50],\n",
    "                                             batch_size=512,\n",
    "                                             activate_decay=True,\n",
    "                                             initial_lr=0.1,\n",
    "                                             max_grad_norm=5.0,\n",
    "                                             show_training=False,\n",
    "                                             freq_summary=1000,\n",
    "                                             buffer_s=-1,\n",
    "                                             initial_pulls=2,\n",
    "                                             reset_lr=True,\n",
    "                                             lr_decay_rate=0.5,\n",
    "                                             training_freq=10,\n",
    "                                             training_freq_network=50,\n",
    "                                             training_epochs=100,\n",
    "                                             a0=6,\n",
    "                                             b0=6,\n",
    "                                             lambda_prior=0.25)\n",
    "\n",
    "hparams_pnoise = tf.contrib.training.HParams(num_actions=num_actions,\n",
    "                                           context_dim=context_dim,\n",
    "                                           init_scale=0.3,\n",
    "                                           activation=tf.nn.relu,\n",
    "                                           layer_sizes=[50],\n",
    "                                           batch_size=512,\n",
    "                                           activate_decay=True,\n",
    "                                           initial_lr=0.1,\n",
    "                                           max_grad_norm=5.0,\n",
    "                                           show_training=False,\n",
    "                                           freq_summary=1000,\n",
    "                                           buffer_s=-1,\n",
    "                                           initial_pulls=2,\n",
    "                                           optimizer='RMS',\n",
    "                                           reset_lr=True,\n",
    "                                           lr_decay_rate=0.5,\n",
    "                                           training_freq=50,\n",
    "                                           training_epochs=100,\n",
    "                                           noise_std=0.05,\n",
    "                                           eps=0.1,\n",
    "                                           d_samples=300,\n",
    "                                          )\n",
    "\n",
    "hparams_alpha_div = tf.contrib.training.HParams(num_actions=num_actions,\n",
    "                                              context_dim=context_dim,\n",
    "                                              init_scale=0.3,\n",
    "                                              activation=tf.nn.relu,\n",
    "                                              layer_sizes=[50],\n",
    "                                              batch_size=512,\n",
    "                                              activate_decay=True,\n",
    "                                              initial_lr=0.1,\n",
    "                                              max_grad_norm=5.0,\n",
    "                                              show_training=False,\n",
    "                                              freq_summary=1000,\n",
    "                                              buffer_s=-1,\n",
    "                                              initial_pulls=2,\n",
    "                                              optimizer='RMS',\n",
    "                                              use_sigma_exp_transform=True,\n",
    "                                              cleared_times_trained=10,\n",
    "                                              initial_training_steps=100,\n",
    "                                              noise_sigma=0.1,\n",
    "                                              reset_lr=False,\n",
    "                                              training_freq=50,\n",
    "                                              training_epochs=100,\n",
    "                                              alpha=1.0,\n",
    "                                              k=20,\n",
    "                                              prior_variance=0.1)\n",
    "\n",
    "hparams_gp = tf.contrib.training.HParams(num_actions=num_actions,\n",
    "                                       num_outputs=num_actions,\n",
    "                                       context_dim=context_dim,\n",
    "                                       reset_lr=False,\n",
    "                                       learn_embeddings=True,\n",
    "                                       max_num_points=1000,\n",
    "                                       show_training=False,\n",
    "                                       freq_summary=1000,\n",
    "                                       batch_size=512,\n",
    "                                       keep_fixed_after_max_obs=True,\n",
    "                                       training_freq=50,\n",
    "                                       initial_pulls=2,\n",
    "                                       training_epochs=100,\n",
    "                                       lr=0.01,\n",
    "                                       buffer_s=-1,\n",
    "                                       initial_lr=0.001,\n",
    "                                       lr_decay_rate=0.0,\n",
    "                                       optimizer='RMS',\n",
    "                                       task_latent_dim=5,\n",
    "                                       activate_decay=False)\n",
    "### Create hyper-parameter configurations for other algorithms\n",
    "[...]\n",
    "\n",
    "algos = [\n",
    "  UniformSampling('Uniform Sampling', hparams),\n",
    "  PosteriorBNNSampling('Dropout', hparams_dropout, 'RMSProp'),\n",
    "  PosteriorBNNSampling('BBB', hparams_bbb, 'Variational'),\n",
    "  NeuralLinearPosteriorSampling('NeuralLinear', hparams_nlinear),\n",
    "  LinearFullPosteriorSampling('LinFullPost', hparams_linear),\n",
    "  BootstrappedBNNSampling('BootRMS', hparams_boot),\n",
    "  ParameterNoiseSampling('ParamNoise', hparams_pnoise),\n",
    "]\n",
    "\n",
    "# Run contextual bandit problem\n",
    "t_init = time.time()\n",
    "results = run_contextual_bandit(context_dim, num_actions, dataset, algos)\n",
    "_, h_rewards = results\n",
    "\n",
    "# Display results\n",
    "display_results(algos, opt_rewards, opt_actions, h_rewards, t_init, data_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
