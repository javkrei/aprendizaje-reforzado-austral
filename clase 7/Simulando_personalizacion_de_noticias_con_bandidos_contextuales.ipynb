{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulación de personalización de noticias usando Bandidos Contextuales\n",
    "\n",
    "Vamos a simular un escenario en que debemos personalizar el contenido de un sitio, usando Bandidos Contextuales (CB). El objetivo es maximizar el Click Through Rate (CTR)\n",
    "\n",
    "Recordemos CB, cada datapoint tiene 4 componentes: \n",
    "\n",
    "- Contexto\n",
    "- Acción\n",
    "- Probabilidad de tomar la acción\n",
    "- Recompensa/costo recibida\n",
    "\n",
    "\n",
    "En el simulador necesitamos generar contexto, obtener una acción para cada contexto y también simular la recompensa.\n",
    "\n",
    "En el simulador, el objetivo es maximizar la recompensa (CTR) o minizar la pérdida (-CTR).\n",
    "\n",
    "\n",
    "- Tenemos dos visitantes al sitio, 'Tom' y 'Anna'\n",
    "- Cada un do ellos puede visitar el sitio a la mañana o a la tarde\n",
    "\n",
    "El **contexto** es entonces (usuario, tiempo_del_dia)\n",
    "\n",
    "Tenemos la opción de recomendar una variedad de artículos. Las acciones son entonces las diferentes opciones: \"politica\", \"deportes\", \"musica\", \"comida\", \"finanzas\", \"salud\" y \"queso\"\n",
    "\n",
    "La **recompensa** es si cliquean o no en el artículo: 'click' o 'no click'\n",
    "\n",
    "Importamos algunas cosas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vowpalwabbit import pyvw\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simular la recompensa\n",
    "\n",
    "En el mundo real vamos a tener que aprender las preferencias de Tom y Anna por los artículos observando sus interacciones. Como esta es una simulación vamos a definir un perfil de preferencias de Tom y Anna. La recompensa que proveemos a un agente (que aprende) va a respetar este perfil de preferencia. Nuestra esperanza es que el agente tome cada vez mejores decisiones a medida que observa más ejemplo y maximice así la recompensa.\n",
    "\n",
    "Tambié vamos a modificar la función de recompensa de diferentes maneras (así no es estacionario el problema) y ver si el agente lo aprende. Vamos a compara el CTR con aprendizaje y sin aprendizaje.\n",
    "\n",
    "Vowpal Wabbit optimiza la función de costo, **que es una recompensa negativa**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como VW trata de minizar costo/pérdida, usamos -recompensa\n",
    "USER_LIKED_ARTICLE = -1.0\n",
    "USER_DISLIKED_ARTICLE = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de recompensa de abajo especifíca que a Tom le gusta leer de política a la mañana y música a la tarde mientras que Anna prefiere leer de deportes a la mañana y de política a la tarde. Si el agente recomienda un artículo que se aliñea con la función de recompensa entonces recibimos una recompensa positiva. En este caso simulado, un click."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost(context,action):\n",
    "    if context['user'] == \"Tom\":\n",
    "        if context['time_of_day'] == \"morning\" and action == 'politics':\n",
    "            return USER_LIKED_ARTICLE\n",
    "        elif context['time_of_day'] == \"afternoon\" and action == 'music':\n",
    "            return USER_LIKED_ARTICLE\n",
    "        else:\n",
    "            return USER_DISLIKED_ARTICLE\n",
    "    elif context['user'] == \"Anna\":\n",
    "        if context['time_of_day'] == \"morning\" and action == 'sports':\n",
    "            return USER_LIKED_ARTICLE\n",
    "        elif context['time_of_day'] == \"afternoon\" and action == 'politics':\n",
    "            return USER_LIKED_ARTICLE\n",
    "        else:\n",
    "            return USER_DISLIKED_ARTICLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## El formato de Vowpal Wabbit\n",
    "\n",
    "Hay algunas cosas que hay que hacerle al input para que VW lo entienda. La siguiente función que se ocupa de convertir desde el contexto, lista de artículos y recompensa a un formato que VW entienda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función modifica la tupla (contexto, acción, costo, probabilidad) a un formato amigable a VW\n",
    "def to_vw_example_format(context, actions, cb_label = None):\n",
    "    if cb_label is not None:\n",
    "        chosen_action, cost, prob = cb_label\n",
    "    example_string = \"\"\n",
    "    example_string += \"shared |User user={} time_of_day={}\\n\".format(context[\"user\"], context[\"time_of_day\"])\n",
    "    for action in actions:\n",
    "        if cb_label is not None and action == chosen_action:\n",
    "            example_string += \"0:{}:{} \".format(cost, prob)\n",
    "        example_string += \"|Action article={} \\n\".format(action)\n",
    "    #remover el último pasaje de línea (\\n)\n",
    "    return example_string[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entener qué está pasando veamos un ejemplo. Aqui, es de mañana y el usuario es Tom. Hay cuatro artículos posibles. Así que en el formato de VW hay una línea que empiezaz con shared (o sea el contexto que fue 'compartido'), segudio de cuatro líneas correspondientes a cada artículo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = {\"user\":\"Tom\",\"time_of_day\":\"morning\"}\n",
    "actions = [\"politics\", \"sports\", \"music\", \"food\"]\n",
    "\n",
    "print(to_vw_example_format(context,actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obteniendo una decisión\n",
    "\n",
    "Cuando llamos a VW obtenemos una distribución de probabilidad sobre acciones como salida. En cada índice está la probabilidad de tomar la acción correspondiente a ese índice. Para tomar una decisión 'sampleamos' acciones con esas probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_custom_pmf(pmf):\n",
    "    total = sum(pmf)\n",
    "    scale = 1/total\n",
    "    pmf = [x * scale for x in pmf]\n",
    "    index = np.random.choice(range(len(pmf)), p=pmf)\n",
    "    return index, pmf[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all of the information we need to choose an action for a specific user and context. To use VW to achieve this, we will do the following:\n",
    "\n",
    "1. We convert our context and actions into the text format we need\n",
    "2. We pass this example to vw and get the pmf out\n",
    "3. Now, we sample this pmf to get what article we will end up showing\n",
    "4. Finally we return the article chosen, and the probability of choosing it (we are going to need the probability when we learn form this example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(vw, context, actions):\n",
    "    vw_text_example = to_vw_example_format(context,actions)\n",
    "    pmf = vw.predict(vw_text_example)\n",
    "    chosen_action_index, prob = sample_custom_pmf(pmf)\n",
    "    return actions[chosen_action_index], prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup de la simulación\n",
    "\n",
    "Ahora simulemos el mundo de Tom y Anna. El escenario es que van a un sitio y les muestran un artículo. Vamos a elegir entre Tom y Anna aleatoriamente de manera uniforme y también el tiempo de visita de manera uniforme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ['Tom', 'Anna']\n",
    "times_of_day = ['morning', 'afternoon']\n",
    "actions = [\"politics\", \"sports\", \"music\", \"food\", \"finance\", \"health\", \"camping\"]\n",
    "\n",
    "def choose_user(users):\n",
    "    return random.choice(users)\n",
    "\n",
    "def choose_time_of_day(times_of_day):\n",
    "    return random.choice(times_of_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos un agente de CB in VW y simulamos las visitas de Tom y Anna durante `num_iterations` veces. En cada visita:\n",
    "\n",
    "We will instantiate a CB learner in VW and then simulate Tom and Anna's website visits `num_iterations` number of times. In each visit, we:\n",
    "\n",
    "1. Decidimos entre Tom y Anna\n",
    "2. Decidimos el tiempo del día\n",
    "3. Pasamos un contexto (usuario, tiempo del dia) al agente para obtener una acción (una recomendación de artículo y la probabilidad)\n",
    "4. Recibimos una recompensa (si el usuario cliqueó). Recordar que el costo es la recompensa negativa.\n",
    "5. Formatamos el contextio, acción, probabilidad y recompensa en e formato VW.\n",
    "6. Aprendemos del ejemplo.\n",
    "\n",
    "Eso es igual para todas las simulaciones más abajo y se usa la función `run_simulation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(vw, num_iterations, users, times_of_day, actions, cost_function, do_learn = True):\n",
    "    cost_sum = 0.\n",
    "    ctr = []\n",
    "\n",
    "    for i in range(1, num_iterations+1):\n",
    "        # 1. En cada simulación elegir el usuario\n",
    "        user = choose_user(users)\n",
    "        # 2. Elegir el tiempo del día para el usuario\n",
    "        time_of_day = choose_time_of_day(times_of_day)\n",
    "\n",
    "        # 3. pasar el contexto a VW para obtener la acción\n",
    "        context = {'user': user, 'time_of_day': time_of_day}\n",
    "        action, prob = get_action(vw, context, actions)\n",
    "\n",
    "        # 4. obtener el costo de la acción tomada\n",
    "        cost = cost_function(context, action)\n",
    "        cost_sum += cost\n",
    "\n",
    "        if do_learn:\n",
    "            # 5. informar a VW de lo que ocurrió y para poder aprender\n",
    "            vw_format = vw.parse(to_vw_example_format(context, actions, (action, cost, prob)),pyvw.vw.lContextualBandit)\n",
    "            # 6. aprender\n",
    "            vw.learn(vw_format)\n",
    "\n",
    "        # hacemos -costo para obtener CTR\n",
    "        ctr.append(-1*cost_sum/i)\n",
    "\n",
    "    return ctr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos poder visualizar lo que ocuerre, así qye vamos a plotear el CTR en cada iteración de la simulación. Si VW toma acciones que obtienen recompensas el CTR va a ser más alto. La función más abajo plotea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ctr(num_iterations, ctr):\n",
    "    plt.plot(range(1,num_iterations+1), ctr)\n",
    "    plt.xlabel('num_iterations', fontsize=14)\n",
    "    plt.ylabel('ctr', fontsize=14)\n",
    "    plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escenario 1\n",
    "\n",
    "Vamos a usar la función de recompensa `get_cost` y asumir que Tom y Anna no cambian sus preferencias a lo largo del tiempo y ver qué pasa con el CTR mientras se aprende. También comparamos con el baseline cuando no se aprende.\n",
    "\n",
    "\n",
    "### Con aprendizaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciar agente en VW\n",
    "vw = pyvw.vw(\"--cb_explore_adf -q UA --quiet --epsilon 0.2\")\n",
    "\n",
    "num_iterations = 1000\n",
    "ctr = run_simulation(vw, num_iterations, users, times_of_day, actions, get_cost)\n",
    "\n",
    "plot_ctr(num_iterations, ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nota: interacciones\n",
    "Uno de los argumentos que pasamos a VW es `-q UA`. Esto le dice a VW que cree features adicionales que son las features del usuario y acción multiplicadas. Esto permite aprender de interacciones entre tiempos del día/usuario y acciones. Si no el aprendizaje con VW no funciona. Se ve abajo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar VW per sin -q\n",
    "vw = pyvw.vw(\"--cb_explore_adf --quiet --epsilon 0.2\")\n",
    "\n",
    "num_iterations = 1000\n",
    "ctr = run_simulation(vw, num_iterations, users, times_of_day, actions, get_cost)\n",
    "\n",
    "plot_ctr(num_iterations, ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sin aprendizaje\n",
    "Hagamos lo mismo de nuevo (pero con `-q`, y esta vez sin aprender, el CTR no mejora y oscilar alrededor de 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar agente en VW\n",
    "vw = pyvw.vw(\"--cb_explore_adf -q UA --quiet --epsilon 0.2\")\n",
    "\n",
    "num_iterations = 1000\n",
    "ctr = run_simulation(vw, num_iterations, users, times_of_day, actions, get_cost, do_learn=False)\n",
    "\n",
    "plot_ctr(num_iterations, ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escenario 2\n",
    "\n",
    "En el mundo real las preferencias de la gente cambian con el tiempo. Ahora simulamos eso incorporando dos funciones de costo. Y cambiamos una por la otra en la mitad del experimento.\n",
    "\n",
    "### Tom\n",
    "\n",
    "| | `get_cost` | `get_cost_new1` |\n",
    "|:---|:---:|:---:|\n",
    "| **Mañana** | Politics | Politics |\n",
    "| **Tarde** | Music | Sports |\n",
    "\n",
    "### Anna\n",
    "\n",
    "| | `get_cost` | `get_cost_new1`  |\n",
    "|:---|:---:|:---:|\n",
    "| **Mañana** | Sports | Sports |\n",
    "| **Tarde** | Politics | Sports |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_new1(context,action):\n",
    "    if context['user'] == \"Tom\":\n",
    "        if context['time_of_day'] == \"morning\" and action == 'politics':\n",
    "            return USER_LIKED_ARTICLE\n",
    "        elif context['time_of_day'] == \"afternoon\" and action == 'sports':\n",
    "            return USER_LIKED_ARTICLE\n",
    "        else:\n",
    "            return USER_DISLIKED_ARTICLE\n",
    "    elif context['user'] == \"Anna\":\n",
    "        if context['time_of_day'] == \"morning\" and action == 'sports':\n",
    "            return USER_LIKED_ARTICLE\n",
    "        elif context['time_of_day'] == \"afternoon\" and action == 'sports':\n",
    "            return USER_LIKED_ARTICLE\n",
    "        else:\n",
    "            return USER_DISLIKED_ARTICLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacerlo simple mostramos el efecto de la función de costo que estamos cambiando modificamos la función `run_simulation`. Permite aceptar una lista de funciones de costo y va a cambiar de una a otra por vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation_multiple_cost_functions(vw, num_iterations, users, times_of_day, actions, cost_functions, do_learn = True):\n",
    "    cost_sum = 0.\n",
    "    ctr = []\n",
    "\n",
    "    start_counter = 1\n",
    "    end_counter = start_counter + num_iterations\n",
    "    for cost_function in cost_functions:\n",
    "        for i in range(start_counter, end_counter):\n",
    "            # 1.en cada simulacion elegir el usuario\n",
    "            user = choose_user(users)\n",
    "            # 2. elegir el tiempo del dia para cada usuario\n",
    "            time_of_day = choose_time_of_day(times_of_day)\n",
    "\n",
    "            # construir contexto\n",
    "            context = {'user': user, 'time_of_day': time_of_day}\n",
    "\n",
    "            # 3. obtener accion\n",
    "            action, prob = get_action(vw, context, actions)\n",
    "\n",
    "            # 4. obtener costo de la accion tomada\n",
    "            cost = cost_function(context, action)\n",
    "            cost_sum += cost\n",
    "\n",
    "            if do_learn:\n",
    "                # 5. informar a VW de lo que pasó\n",
    "                vw_format = vw.parse(to_vw_example_format(context, actions, (action, cost, prob)),pyvw.vw.lContextualBandit)\n",
    "                # 6. aprender\n",
    "                vw.learn(vw_format)\n",
    "\n",
    "            # hacemos -costo para obtener CTR\n",
    "            ctr.append(-1*cost_sum/i)\n",
    "        start_counter = end_counter\n",
    "        end_counter = start_counter + num_iterations\n",
    "    return ctr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con aprendizaje\n",
    "\n",
    "Ahora prendemos la segunda función de recompensa luego de algunos ejemplos. Esperamos ver cómo el agente aprende este cambio de situación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usar la primera función de recompensa inicialmente y luego cambiar a la segunda\n",
    "\n",
    "# instanciar agente en VW\n",
    "vw = pyvw.vw(\"--cb_explore_adf -q UA --quiet --epsilon 0.2\")\n",
    "\n",
    "num_iterations_per_cost_func = 1000\n",
    "cost_functions = [get_cost, get_cost_new1]\n",
    "total_iterations = num_iterations_per_cost_func * len(cost_functions)\n",
    "\n",
    "ctr = run_simulation_multiple_cost_functions(vw, num_iterations_per_cost_func, users, times_of_day, actions, cost_functions)\n",
    "\n",
    "plot_ctr(total_iterations, ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Nota:** El pico inicial de CTR depende de las recompensas iniciales en los primeros ejemplos. Esto es un aleatorio en cada corrida.\n",
    "\n",
    "### Sin aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no aprender\n",
    "# usar la primera función de recompensa inicialmente y luego cambiar a la segunda\n",
    "\n",
    "# instanciar agente en VW\n",
    "vw = pyvw.vw(\"--cb_explore_adf -q UA --quiet --epsilon 0.2\")\n",
    "\n",
    "num_iterations_per_cost_func = 1000\n",
    "cost_functions = [get_cost, get_cost_new1]\n",
    "total_iterations = num_iterations_per_cost_func * len(cost_functions)\n",
    "\n",
    "ctr = run_simulation_multiple_cost_functions(vw, num_iterations_per_cost_func, users, times_of_day, actions, cost_functions, do_learn=False)\n",
    "plot_ctr(total_iterations, ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escenario 3\n",
    "En este escenario empezamos a recompensar acciones que nunca vieron una recompensa previamente cuando cambiamos la función de costo.\n",
    "\n",
    "### Tom\n",
    "\n",
    "| | `get_cost` | `get_cost_new2` |\n",
    "|:---|:---:|:---:|\n",
    "| **Mañana** | Politics |  Politics|\n",
    "| **Tarde** | Music |   Food |\n",
    "\n",
    "### Anna\n",
    "\n",
    "| | `get_cost` | `get_cost_new2` |\n",
    "|:---|:---:|:---:|\n",
    "| **Mañana** | Sports | Food|\n",
    "| **Tarde** | Politics |  Food |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_new2(context,action):\n",
    "    if context['user'] == \"Tom\":\n",
    "        if context['time_of_day'] == \"morning\" and action == 'politics':\n",
    "            return USER_LIKED_ARTICLE\n",
    "        elif context['time_of_day'] == \"afternoon\" and action == 'food':\n",
    "            return USER_LIKED_ARTICLE\n",
    "        else:\n",
    "            return USER_DISLIKED_ARTICLE\n",
    "    elif context['user'] == \"Anna\":\n",
    "        if context['time_of_day'] == \"morning\" and action == 'food':\n",
    "            return USER_LIKED_ARTICLE\n",
    "        elif context['time_of_day'] == \"afternoon\" and action == 'food':\n",
    "            return USER_LIKED_ARTICLE\n",
    "        else:\n",
    "            return USER_DISLIKED_ARTICLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Con aprendizaje\n",
    "Ahora la función  de recompensa funciona con espacio de acción **diferente** después de un tiempo. Deberíamos ver al agente aprender esto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usar la primera función de recompensa inicialmente y luego cambiar a la tercera\n",
    "\n",
    "# instanciar agente en VW\n",
    "vw = pyvw.vw(\"--cb_explore_adf -q UA --quiet --epsilon 0.2\")\n",
    "\n",
    "num_iterations_per_cost_func = 1000\n",
    "cost_functions = [get_cost, get_cost_new2]\n",
    "total_iterations = num_iterations_per_cost_func * len(cost_functions)\n",
    "\n",
    "ctr = run_simulation_multiple_cost_functions(vw, num_iterations_per_cost_func, users, times_of_day, actions, cost_functions)\n",
    "\n",
    "plot_ctr(total_iterations, ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sin aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no aprender\n",
    "# usar la primera función de recompensa inicialmente y luego cambiar a la tercera\n",
    "\n",
    "# instanciar agente en VW\n",
    "vw = pyvw.vw(\"--cb_explore_adf -q UA --quiet --epsilon 0.2\")\n",
    "\n",
    "num_iterations_per_cost_func = 1000\n",
    "cost_functions = [get_cost, get_cost_new2]\n",
    "total_iterations = num_iterations_per_cost_func * len(cost_functions)\n",
    "\n",
    "ctr = run_simulation_multiple_cost_functions(vw, num_iterations_per_cost_func, users, times_of_day, actions, cost_functions, do_learn=False)\n",
    "\n",
    "plot_ctr(total_iterations, ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "Este tutorial trata de mostrar un escenario real en una situación muy simplificada donde se pueden usar bandidos contextuales. Vimos que se puede tomar un contexto y una serie de acciones y cómo se puede aprender cuáles son óptimas para cada contexto. Vimos que el agente puede aprender rápidamente a los cambios del munco. \n",
    "\n",
    "Este tutorial es muy simplificado, pero VW soporta features de altas dimensiones y diferentes algoritmos de exploración y estrategias evaluación de políticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
