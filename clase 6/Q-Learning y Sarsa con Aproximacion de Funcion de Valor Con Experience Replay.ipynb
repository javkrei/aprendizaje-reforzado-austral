{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El objetivo de este ejercicio es implementar q-learning y sarsa con aproximación de función de valor utilizando experience replay.\n",
    "\n",
    "## Experience replay consiste en lo siguiente:\n",
    "\n",
    "<img src=\"experience_replay.PNG\">\n",
    "\n",
    "## Recordar los targets de sarsa y q-learning respectivamente:\n",
    "\n",
    "## Sarsa:\n",
    "<img src=\"sarsa.PNG\">\n",
    "\n",
    "## Q-Learning:\n",
    "\n",
    "<img src=\"q-learning.PNG\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/javkrei/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/javkrei/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/javkrei/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/javkrei/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/javkrei/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/javkrei/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/javkrei/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/javkrei/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/javkrei/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/javkrei/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/javkrei/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/javkrei/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import sklearn.preprocessing\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\") \n",
    "from lib import plotting\n",
    "\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "env = gym.make(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"\n",
    "    Red Neuronal que aproxima la función de valor estado acción (q-function)\n",
    "    \"\"\"   \n",
    "    def __init__(self, dimS, nA, learning_rate=0.001):\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(dimS,), activation=\"relu\"))\n",
    "        self.model.add(Dense(24, activation=\"relu\"))\n",
    "        self.model.add(Dense(nA, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=learning_rate))\n",
    "     \n",
    "    def update(self, states, q_values, verbose=0):\n",
    "        \"\"\"\n",
    "        Realiza un update de los parámetros de la red neuronal usando un batch de estados y batch de vectores de valores\n",
    "        de la función estado-acción correspondientes\n",
    "        \"\"\"\n",
    "        self.model.fit(states, q_values, verbose=0)\n",
    "    \n",
    "    def predict(self, s):\n",
    "        \"\"\"\n",
    "        Realiza una predicción de la función de valor estado-accion dado el estado\n",
    "        \n",
    "        Argumentos:\n",
    "            s: estado para el cual realizar la predicción\n",
    "            \n",
    "        Retorna:\n",
    "            Un vector con la predicción de la función de valor para todas las accións\n",
    "        \"\"\"\n",
    "        return self.model.predict(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, env, estimator, num_episodes, discount_factor=1.0, \n",
    "                 exploration_max=1.0, exploration_min=0.01, epsilon_decay=0.99, memory_size=1000000, \n",
    "                 batch_size=20, run_online=False, use_qlearning=True):\n",
    "        \"\"\"\n",
    "        Algoritmo q-learning/sarsa con experience replay utilizando aproximación de funciones.\n",
    "\n",
    "        Argumentos de inicialización:\n",
    "            env: ambiente de OpenAI.\n",
    "            estimator: función de aproximación de la función de valor estado-acción.\n",
    "            num_episodes: número de episodios durante los cuales entrenar el estimador.\n",
    "            discount_factor: factor de descuento gama.\n",
    "            exploration_max: probabilidad de tomar una acción aleatoria inicial.\n",
    "            exploration_min: mínima probabilidad de tomar una acción aleatoria.\n",
    "            epsilon_decay: en cada episodio la probabilidad de una acción aleatoria deace por este factor.\n",
    "            run_online: si es verdadero NO usa experience replay\n",
    "            use_qlearning: si es verdadero usa q-learning, si es falso usa sarsa\n",
    "        \"\"\"\n",
    "        self.exploration_rate = exploration_max\n",
    "        self.exploration_min = exploration_min\n",
    "        self.env = env\n",
    "        self.estimator = estimator\n",
    "        self.num_episodes = num_episodes\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.run_online = run_online\n",
    "        self.use_qlearning = use_qlearning\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def decay_exploration_rate(self):\n",
    "        self.exploration_rate *= self.epsilon_decay\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate)\n",
    "        \n",
    "    def policy_fn(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Política epsilon-greedy basada en la función de aproximación actual y la probabilidad de exploración epsilon.\n",
    "\n",
    "        Retorna:\n",
    "            Un vector con la probabilidad de tomar cada acción.\n",
    "        \"\"\"\n",
    "        A = np.ones(env.action_space.n, dtype=float) * self.exploration_rate / env.action_space.n\n",
    "        q_values = self.estimator.predict(state)\n",
    "        best_action = np.argmax(q_values[0])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        \"\"\"\n",
    "        Realiza una corrida de entrenamiento de la función de aproximación dado un batch de experiencia acumulada.\n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        # contienen un batch de 'experiencia' sampleada de la memoria acumulada\n",
    "        batch_q_values = np.zeros((self.batch_size, self.env.action_space.n))\n",
    "        batch_states = np.zeros((self.batch_size, self.env.observation_space.shape[0]))\n",
    "        \n",
    "        for i_batch, (state, action, reward, next_state, terminal) in enumerate(batch):\n",
    "            \n",
    "            q_update = reward\n",
    "            if not terminal:\n",
    "                ## q-learning:\n",
    "                if self.use_qlearning:\n",
    "                    # COMPLETAR!\n",
    "                    # escribir el target de q-learning\n",
    "                    q_update = reward + self.discount_factor * np.max(self.estimator.predict(next_state))\n",
    "                else: \n",
    "                    ## sarsa:\n",
    "                    # COMPLETAR!\n",
    "                    # obtener las probabilidades de las acciones dado la política epsilon-greedy con\n",
    "                    # respecto a la función de valor estado-acción actual y el próximo estado\n",
    "                    next_action_probs = self.policy_fn(state, self.exploration_rate)\n",
    "                    # elegir la próxima acción aleatoriamente según esa distribución\n",
    "                    next_action = np.random.choice(self.env.action_space.n, p=next_action_probs)\n",
    "                    # obtener los valores de la función de valor estado-acción para el próximo estado\n",
    "                    q_values_next = self.estimator(next_state)\n",
    "                    # escribir el target de sarsa para la acción seleccionada\n",
    "                    q_update = reward + self.discount_factor * q_values_next[next_action]\n",
    "            \n",
    "            # COMPLETAR\n",
    "            # valores actuales de la función de valor para este estado:\n",
    "            q_values = self.estimator.predict(state)\n",
    "            # substituir el valor de la acción actual por el target\n",
    "            q_values[0][action] = q_update\n",
    "            \n",
    "            # llenar el array del batch\n",
    "            batch_q_values[i_batch] = q_values\n",
    "            batch_states[i_batch] = state\n",
    "            i_batch += 1\n",
    "        # avanzar en esa la dirección del gradiente dado los estados y los targets en el batch\n",
    "        self.estimator.update(batch_states, batch_q_values, verbose=0)\n",
    "        \n",
    "    def train(self):    \n",
    "        \"\"\"\n",
    "        Realiza Q-Learning o Sarsa con aproximación de la función de valor estado-acción.\n",
    "        Retorna:\n",
    "            Un objeto de tipo EpisodeStats con dos arrays de numpy para la longitud y recompensa acumulada de cada\n",
    "            episodio respectivamente.\n",
    "        \"\"\"\n",
    "        stats = plotting.EpisodeStats(\n",
    "            episode_lengths=np.zeros(self.num_episodes),\n",
    "            episode_rewards=np.zeros(self.num_episodes))    \n",
    "        \n",
    "        for run in range(self.num_episodes):\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, self.env.observation_space.shape[0]])\n",
    "            step = 0\n",
    "            cum_reward = 0.0\n",
    "            \n",
    "            action = np.random.choice(self.env.action_space.n, p=self.policy_fn(state, self.exploration_rate))\n",
    "            \n",
    "            while True:\n",
    "                env.render()\n",
    "                \n",
    "                next_state, reward, terminal, info = env.step(action)\n",
    "                \n",
    "                reward = reward if not terminal else -reward\n",
    "                cum_reward += reward*self.discount_factor**step\n",
    "\n",
    "                next_state = np.reshape(next_state, [1, self.env.observation_space.shape[0]])\n",
    "                \n",
    "                if not terminal:\n",
    "                    next_action = np.random.choice(self.env.action_space.n, p=self.policy_fn(next_state, self.exploration_rate))\n",
    "                else:\n",
    "                    next_action = None\n",
    "                \n",
    "                self.remember(state, action, reward, next_state, terminal)\n",
    "                \n",
    "                if self.run_online:\n",
    "                    # este es el caso en que hacemos un update en cada paso de experiencia:\n",
    "                    q_update = reward\n",
    "                    if not terminal:\n",
    "                        ## q-learning:\n",
    "                        if self.use_qlearning:\n",
    "                            # COMPLETAR!\n",
    "                            # escribir el target de q-learning\n",
    "                            q_update = reward + self.discount_factor * np.max(self.estimator.predict(next_state))\n",
    "                        else: \n",
    "                            ## sarsa:\n",
    "                            # COMPLETAR!\n",
    "                            # obtener las probabilidades de las acciones dado la política epsilon-greedy con\n",
    "                            # respecto a la función de valor estado-acción actual y el próximo estado\n",
    "                            next_action_probs = self.policy_fn(state, self.exploration_rate)\n",
    "                            # elegir la próxima acción aleatoriamente según esa distribución\n",
    "                            next_action = np.random.choice(self.env.action_space.n, p=next_action_probs)\n",
    "                            # obtener los valores de la función de valor estado-acción para el próximo estado\n",
    "                            q_values_next = self.estimator(next_state)\n",
    "                            # escribir el target de sarsa para la acción seleccionada\n",
    "                            q_update = reward + self.discount_factor * q_values_next[next_action]\n",
    "                    # COMPLETAR!\n",
    "                    # valores actuales de la función de valor para este estado:\n",
    "                    q_values = self.estimator.predict(state)\n",
    "                    q_values[0][action] = q_update\n",
    "                    # hacer update en la dirección del gradiente\n",
    "                    self.estimator.update([state], [q_values], verbose=0)\n",
    "                    \n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                \n",
    "                step += 1\n",
    "                if terminal:\n",
    "                    print(\"Episodio: \" + str(run) + \", Exploración: \" + str(self.exploration_rate) + \n",
    "                          \", Recompensa Acumulada: \" + str(cum_reward))\n",
    "                    # Actualizar estadísticas\n",
    "                    stats.episode_rewards[run] = cum_reward\n",
    "                    stats.episode_lengths[run] = step\n",
    "                    break        \n",
    "                    \n",
    "                if not self.run_online:\n",
    "                    self.experience_replay()\n",
    "            self.decay_exploration_rate()\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(env.observation_space.shape[0], env.action_space.n)\n",
    "learner = Learner(env, estimator, 150, run_online=False, use_qlearning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 0, Exploración: 0.22145178723886094, Recompensa Acumulada: 57.0\n",
      "Episodio: 1, Exploración: 0.21923726936647234, Recompensa Acumulada: 79.0\n",
      "Episodio: 2, Exploración: 0.2170448966728076, Recompensa Acumulada: 72.0\n",
      "Episodio: 3, Exploración: 0.21487444770607952, Recompensa Acumulada: 87.0\n",
      "Episodio: 4, Exploración: 0.21272570322901874, Recompensa Acumulada: 105.0\n",
      "Episodio: 5, Exploración: 0.21059844619672854, Recompensa Acumulada: 149.0\n",
      "Episodio: 6, Exploración: 0.20849246173476127, Recompensa Acumulada: 40.0\n",
      "Episodio: 7, Exploración: 0.20640753711741366, Recompensa Acumulada: 72.0\n",
      "Episodio: 8, Exploración: 0.20434346174623952, Recompensa Acumulada: 89.0\n",
      "Episodio: 9, Exploración: 0.20230002712877712, Recompensa Acumulada: 129.0\n",
      "Episodio: 10, Exploración: 0.20027702685748935, Recompensa Acumulada: 119.0\n",
      "Episodio: 11, Exploración: 0.19827425658891445, Recompensa Acumulada: 71.0\n",
      "Episodio: 12, Exploración: 0.1962915140230253, Recompensa Acumulada: 220.0\n",
      "Episodio: 13, Exploración: 0.19432859888279505, Recompensa Acumulada: 129.0\n",
      "Episodio: 14, Exploración: 0.1923853128939671, Recompensa Acumulada: 172.0\n",
      "Episodio: 15, Exploración: 0.19046145976502743, Recompensa Acumulada: 108.0\n",
      "Episodio: 16, Exploración: 0.18855684516737714, Recompensa Acumulada: 117.0\n",
      "Episodio: 17, Exploración: 0.18667127671570335, Recompensa Acumulada: 227.0\n",
      "Episodio: 18, Exploración: 0.18480456394854633, Recompensa Acumulada: 120.0\n",
      "Episodio: 19, Exploración: 0.18295651830906087, Recompensa Acumulada: 254.0\n",
      "Episodio: 20, Exploración: 0.18112695312597027, Recompensa Acumulada: 202.0\n",
      "Episodio: 21, Exploración: 0.17931568359471056, Recompensa Acumulada: 188.0\n",
      "Episodio: 22, Exploración: 0.17752252675876345, Recompensa Acumulada: 204.0\n",
      "Episodio: 23, Exploración: 0.17574730149117582, Recompensa Acumulada: 215.0\n",
      "Episodio: 24, Exploración: 0.17398982847626407, Recompensa Acumulada: 212.0\n",
      "Episodio: 25, Exploración: 0.17224993019150142, Recompensa Acumulada: 178.0\n",
      "Episodio: 26, Exploración: 0.1705274308895864, Recompensa Acumulada: 273.0\n",
      "Episodio: 27, Exploración: 0.16882215658069055, Recompensa Acumulada: 357.0\n",
      "Episodio: 28, Exploración: 0.16713393501488363, Recompensa Acumulada: 283.0\n",
      "Episodio: 29, Exploración: 0.16546259566473479, Recompensa Acumulada: 392.0\n",
      "Episodio: 30, Exploración: 0.16380796970808745, Recompensa Acumulada: 244.0\n",
      "Episodio: 31, Exploración: 0.16216989001100657, Recompensa Acumulada: 221.0\n",
      "Episodio: 32, Exploración: 0.1605481911108965, Recompensa Acumulada: 366.0\n",
      "Episodio: 33, Exploración: 0.15894270919978754, Recompensa Acumulada: 498.0\n",
      "Episodio: 34, Exploración: 0.15735328210778965, Recompensa Acumulada: 373.0\n",
      "Episodio: 35, Exploración: 0.15577974928671176, Recompensa Acumulada: 351.0\n",
      "Episodio: 36, Exploración: 0.15422195179384465, Recompensa Acumulada: 468.0\n",
      "Episodio: 37, Exploración: 0.1526797322759062, Recompensa Acumulada: 498.0\n",
      "Episodio: 38, Exploración: 0.15115293495314713, Recompensa Acumulada: 323.0\n",
      "Episodio: 39, Exploración: 0.14964140560361566, Recompensa Acumulada: 432.0\n",
      "Episodio: 40, Exploración: 0.1481449915475795, Recompensa Acumulada: 432.0\n",
      "Episodio: 41, Exploración: 0.1466635416321037, Recompensa Acumulada: 287.0\n",
      "Episodio: 42, Exploración: 0.14519690621578268, Recompensa Acumulada: 498.0\n",
      "Episodio: 43, Exploración: 0.14374493715362485, Recompensa Acumulada: 498.0\n",
      "Episodio: 44, Exploración: 0.1423074877820886, Recompensa Acumulada: 492.0\n",
      "Episodio: 45, Exploración: 0.1408844129042677, Recompensa Acumulada: 397.0\n",
      "Episodio: 46, Exploración: 0.13947556877522502, Recompensa Acumulada: 284.0\n",
      "Episodio: 47, Exploración: 0.13808081308747278, Recompensa Acumulada: 407.0\n",
      "Episodio: 48, Exploración: 0.13670000495659804, Recompensa Acumulada: 397.0\n",
      "Episodio: 49, Exploración: 0.13533300490703207, Recompensa Acumulada: 498.0\n",
      "Episodio: 50, Exploración: 0.13397967485796175, Recompensa Acumulada: 498.0\n",
      "Episodio: 51, Exploración: 0.13263987810938213, Recompensa Acumulada: 498.0\n",
      "Episodio: 52, Exploración: 0.1313134793282883, Recompensa Acumulada: 498.0\n",
      "Episodio: 53, Exploración: 0.13000034453500542, Recompensa Acumulada: 380.0\n",
      "Episodio: 54, Exploración: 0.12870034108965536, Recompensa Acumulada: 402.0\n",
      "Episodio: 55, Exploración: 0.12741333767875881, Recompensa Acumulada: 248.0\n",
      "Episodio: 56, Exploración: 0.12613920430197123, Recompensa Acumulada: 442.0\n",
      "Episodio: 57, Exploración: 0.12487781225895152, Recompensa Acumulada: 498.0\n",
      "Episodio: 58, Exploración: 0.123629034136362, Recompensa Acumulada: 498.0\n",
      "Episodio: 59, Exploración: 0.12239274379499838, Recompensa Acumulada: 245.0\n",
      "Episodio: 60, Exploración: 0.1211688163570484, Recompensa Acumulada: 223.0\n",
      "Episodio: 61, Exploración: 0.11995712819347792, Recompensa Acumulada: 493.0\n",
      "Episodio: 62, Exploración: 0.11875755691154315, Recompensa Acumulada: 498.0\n",
      "Episodio: 63, Exploración: 0.11756998134242772, Recompensa Acumulada: 485.0\n",
      "Episodio: 64, Exploración: 0.11639428152900344, Recompensa Acumulada: 498.0\n",
      "Episodio: 65, Exploración: 0.11523033871371341, Recompensa Acumulada: 263.0\n",
      "Episodio: 66, Exploración: 0.11407803532657627, Recompensa Acumulada: 409.0\n",
      "Episodio: 67, Exploración: 0.11293725497331052, Recompensa Acumulada: 444.0\n",
      "Episodio: 68, Exploración: 0.1118078824235774, Recompensa Acumulada: 498.0\n",
      "Episodio: 69, Exploración: 0.11068980359934164, Recompensa Acumulada: 498.0\n",
      "Episodio: 70, Exploración: 0.10958290556334822, Recompensa Acumulada: 324.0\n",
      "Episodio: 71, Exploración: 0.10848707650771475, Recompensa Acumulada: 498.0\n",
      "Episodio: 72, Exploración: 0.1074022057426376, Recompensa Acumulada: 498.0\n",
      "Episodio: 73, Exploración: 0.10632818368521123, Recompensa Acumulada: 498.0\n",
      "Episodio: 74, Exploración: 0.10526490184835911, Recompensa Acumulada: 498.0\n",
      "Episodio: 75, Exploración: 0.10421225282987552, Recompensa Acumulada: 498.0\n",
      "Episodio: 76, Exploración: 0.10317013030157676, Recompensa Acumulada: 498.0\n",
      "Episodio: 77, Exploración: 0.10213842899856099, Recompensa Acumulada: 238.0\n",
      "Episodio: 78, Exploración: 0.10111704470857538, Recompensa Acumulada: 311.0\n",
      "Episodio: 79, Exploración: 0.10010587426148963, Recompensa Acumulada: 340.0\n",
      "Episodio: 80, Exploración: 0.09910481551887473, Recompensa Acumulada: 250.0\n",
      "Episodio: 81, Exploración: 0.09811376736368599, Recompensa Acumulada: 338.0\n",
      "Episodio: 82, Exploración: 0.09713262969004913, Recompensa Acumulada: 340.0\n",
      "Episodio: 83, Exploración: 0.09616130339314863, Recompensa Acumulada: 482.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7055674170ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-b9e52a5c47cc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_online\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay_exploration_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-b9e52a5c47cc>\u001b[0m in \u001b[0;36mexperience_replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# COMPLETAR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;31m# valores actuales de la función de valor para este estado:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;31m# substituir el valor de la acción actual por el target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mq_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e273f79c7c37>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mUn\u001b[0m \u001b[0mvector\u001b[0m \u001b[0mcon\u001b[0m \u001b[0mla\u001b[0m \u001b[0mpredicción\u001b[0m \u001b[0mde\u001b[0m \u001b[0mla\u001b[0m \u001b[0mfunción\u001b[0m \u001b[0mde\u001b[0m \u001b[0mvalor\u001b[0m \u001b[0mpara\u001b[0m \u001b[0mtodas\u001b[0m \u001b[0mlas\u001b[0m \u001b[0maccións\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \"\"\"\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3251\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3253\u001b[0;31m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3254\u001b[0m     \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3255\u001b[0m     \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m(op_input_list)\u001b[0m\n\u001b[1;32m    460\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stats = learner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_episode_stats(stats, smoothing_window=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
