{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El objetivo de este ejercicio es implementar q-learning y sarsa con aproximación de función de valor utilizando experience replay.\n",
    "\n",
    "## Experience replay consiste en lo siguiente:\n",
    "\n",
    "<img src=\"experience_replay.PNG\">\n",
    "\n",
    "## Recordar los targets de sarsa y q-learning respectivamente:\n",
    "\n",
    "## Sarsa:\n",
    "<img src=\"sarsa.PNG\">\n",
    "\n",
    "## Q-Learning:\n",
    "\n",
    "<img src=\"q-learning.PNG\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import sklearn.preprocessing\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\") \n",
    "from lib import plotting\n",
    "\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "env = gym.make(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"\n",
    "    Red Neuronal que aproxima la función de valor estado acción (q-function)\n",
    "    \"\"\"   \n",
    "    def __init__(self, dimS, nA, learning_rate=0.001):\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(dimS,), activation=\"relu\"))\n",
    "        self.model.add(Dense(24, activation=\"relu\"))\n",
    "        self.model.add(Dense(nA, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=learning_rate))\n",
    "     \n",
    "    def update(self, states, q_values, verbose=0):\n",
    "        \"\"\"\n",
    "        Realiza un update de los parámetros de la red neuronal usando un batch de estados y batch de vectores de valores\n",
    "        de la función estado-acción correspondientes\n",
    "        \"\"\"\n",
    "        self.model.fit(states, q_values, verbose=0)\n",
    "    \n",
    "    def predict(self, s):\n",
    "        \"\"\"\n",
    "        Realiza una predicción de la función de valor estado-accion dado el estado\n",
    "        \n",
    "        Argumentos:\n",
    "            s: estado para el cual realizar la predicción\n",
    "            \n",
    "        Retorna:\n",
    "            Un vector con la predicción de la función de valor para todas las accións\n",
    "        \"\"\"\n",
    "        return self.model.predict(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, env, estimator, num_episodes, discount_factor=1.0, \n",
    "                 exploration_max=1.0, exploration_min=0.01, epsilon_decay=0.99, memory_size=1000000, \n",
    "                 batch_size=20, run_online=False, use_qlearning=True):\n",
    "        \"\"\"\n",
    "        Algoritmo q-learning/sarsa con experience replay utilizando aproximación de funciones.\n",
    "\n",
    "        Argumentos de inicialización:\n",
    "            env: ambiente de OpenAI.\n",
    "            estimator: función de aproximación de la función de valor estado-acción.\n",
    "            num_episodes: número de episodios durante los cuales entrenar el estimador.\n",
    "            discount_factor: factor de descuento gama.\n",
    "            exploration_max: probabilidad de tomar una acción aleatoria inicial.\n",
    "            exploration_min: mínima probabilidad de tomar una acción aleatoria.\n",
    "            epsilon_decay: en cada episodio la probabilidad de una acción aleatoria deace por este factor.\n",
    "            run_online: si es verdadero NO usa experience replay\n",
    "            use_qlearning: si es verdadero usa q-learning, si es falso usa sarsa\n",
    "        \"\"\"\n",
    "        self.exploration_rate = exploration_max\n",
    "        self.exploration_min = exploration_min\n",
    "        self.env = env\n",
    "        self.estimator = estimator\n",
    "        self.num_episodes = num_episodes\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.run_online = run_online\n",
    "        self.use_qlearning = use_qlearning\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def decay_exploration_rate(self):\n",
    "        self.exploration_rate *= self.epsilon_decay\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate)\n",
    "        \n",
    "    def policy_fn(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Política epsilon-greedy basada en la función de aproximación actual y la probabilidad de exploración epsilon.\n",
    "\n",
    "        Retorna:\n",
    "            Un vector con la probabilidad de tomar cada acción.\n",
    "        \"\"\"\n",
    "        A = np.ones(env.action_space.n, dtype=float) * self.exploration_rate / env.action_space.n\n",
    "        q_values = self.estimator.predict(state)\n",
    "        best_action = np.argmax(q_values[0])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        \"\"\"\n",
    "        Realiza una corrida de entrenamiento de la función de aproximación dado un batch de experiencia acumulada.\n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        # contienen un batch de 'experiencia' sampleada de la memoria acumulada\n",
    "        batch_q_values = np.zeros((self.batch_size, self.env.action_space.n))\n",
    "        batch_states = np.zeros((self.batch_size, self.env.observation_space.shape[0]))\n",
    "        \n",
    "        for i_batch, (state, action, reward, next_state, terminal) in enumerate(batch):\n",
    "            \n",
    "            q_update = reward\n",
    "            if not terminal:\n",
    "                ## q-learning:\n",
    "                if self.use_qlearning:\n",
    "                    # COMPLETAR!\n",
    "                    # escribir el target de q-learning\n",
    "                    q_update = \n",
    "                else: \n",
    "                    ## sarsa:\n",
    "                    # COMPLETAR!\n",
    "                    # obtener las probabilidades de las acciones dado la política epsilon-greedy con\n",
    "                    # respecto a la función de valor estado-acción actual y el próximo estado\n",
    "                    next_action_probs = \n",
    "                    # elegir la próxima acción aleatoriamente según esa distribución\n",
    "                    next_action = \n",
    "                    # obtener los valores de la función de valor estado-acción para el próximo estado\n",
    "                    q_values_next = \n",
    "                    # escribir el target de sarsa para la acción seleccionada\n",
    "                    q_update = \n",
    "            \n",
    "            # COMPLETAR\n",
    "            # valores actuales de la función de valor para este estado:\n",
    "            q_values = \n",
    "            # substituir el valor de la acción actual por el target\n",
    "            q_values[0][action] = q_update\n",
    "            \n",
    "            # llenar el array del batch\n",
    "            batch_q_values[i_batch] = q_values\n",
    "            batch_states[i_batch] = state\n",
    "            i_batch += 1\n",
    "        # avanzar en esa la dirección del gradiente dado los estados y los targets en el batch\n",
    "        self.estimator.update(batch_states, batch_q_values, verbose=0)\n",
    "        \n",
    "    def train(self):    \n",
    "        \"\"\"\n",
    "        Realiza Q-Learning o Sarsa con aproximación de la función de valor estado-acción.\n",
    "        Retorna:\n",
    "            Un objeto de tipo EpisodeStats con dos arrays de numpy para la longitud y recompensa acumulada de cada\n",
    "            episodio respectivamente.\n",
    "        \"\"\"\n",
    "        stats = plotting.EpisodeStats(\n",
    "            episode_lengths=np.zeros(self.num_episodes),\n",
    "            episode_rewards=np.zeros(self.num_episodes))    \n",
    "        \n",
    "        for run in range(self.num_episodes):\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, self.env.observation_space.shape[0]])\n",
    "            step = 0\n",
    "            cum_reward = 0.0\n",
    "            \n",
    "            action = np.random.choice(self.env.action_space.n, p=self.policy_fn(state, self.exploration_rate))\n",
    "            \n",
    "            while True:\n",
    "                env.render()\n",
    "                \n",
    "                next_state, reward, terminal, info = env.step(action)\n",
    "                \n",
    "                reward = reward if not terminal else -reward\n",
    "                cum_reward += reward*self.discount_factor**step\n",
    "\n",
    "                next_state = np.reshape(next_state, [1, self.env.observation_space.shape[0]])\n",
    "                \n",
    "                if not terminal:\n",
    "                    next_action = np.random.choice(self.env.action_space.n, p=self.policy_fn(next_state, self.exploration_rate))\n",
    "                else:\n",
    "                    next_action = None\n",
    "                \n",
    "                self.remember(state, action, reward, next_state, terminal)\n",
    "                \n",
    "                if self.run_online:\n",
    "                    # este es el caso en que hacemos un update en cada paso de experiencia:\n",
    "                    q_update = reward\n",
    "                    if not terminal:\n",
    "                        ## q-learning:\n",
    "                        if self.use_qlearning:\n",
    "                            # COMPLETAR!\n",
    "                            # escribir el target de q-learning\n",
    "                            q_update = \n",
    "                        else: \n",
    "                        ## sarsa:\n",
    "                        # COMPLETAR!\n",
    "                            # obtener las probabilidades de las acciones dado la política epsilon-greedy con\n",
    "                            # respecto a la función de valor estado-acción actual y el próximo estado\n",
    "                            next_action_probs = \n",
    "                            # elegir la próxima acción aleatoriamente según esa distribución\n",
    "                            next_action = \n",
    "                            # obtener los valores de la función de valor estado-acción para el próximo estado\n",
    "                            q_values_next = \n",
    "                            # escribir el target de sarsa para la acción seleccionada\n",
    "                            q_update = \n",
    "                    # COMPLETAR!\n",
    "                    # valores actuales de la función de valor para este estado:\n",
    "                    q_values = \n",
    "                    q_values[0][action] = q_update\n",
    "                    # hacer update en la dirección del gradiente\n",
    "                    self.estimator.update([state], [q_values], verbose=0)\n",
    "                    \n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                \n",
    "                step += 1\n",
    "                if terminal:\n",
    "                    print(\"Episodio: \" + str(run) + \", Exploración: \" + str(self.exploration_rate) + \n",
    "                          \", Recompensa Acumulada: \" + str(cum_reward))\n",
    "                    # Actualizar estadísticas\n",
    "                    stats.episode_rewards[run] = cum_reward\n",
    "                    stats.episode_lengths[run] = step\n",
    "                    break        \n",
    "                    \n",
    "                if not self.run_online:\n",
    "                    self.experience_replay()\n",
    "            self.decay_exploration_rate()\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(env.observation_space.shape[0], env.action_space.n)\n",
    "learner = Learner(env, estimator, 150, run_online=False, use_qlearning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = learner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_episode_stats(stats, smoothing_window=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
