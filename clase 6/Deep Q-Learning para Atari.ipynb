{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, Lambda\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "import sklearn.preprocessing\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\") \n",
    "from lib import plotting\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "ENV_NAME = \"Breakout-v0\"\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "# fuentes:\n",
    "# https://github.com/rohitgirdhar/Deep-Q-Networks/\n",
    "# https://github.com/keon/deep-q-learning/blob/master/dqn.py\n",
    "# https://github.com/AdamStelmaszczyk/dqn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"\n",
    "    Red Neuronal que aproxima la función de valor estado acción (q-function)\n",
    "    \"\"\"   \n",
    "    def __init__(self, env, frame_history=4, learning_rate=0.001):        \n",
    "        n_actions = env.action_space.n\n",
    "        obs_shape = (84,84,frame_history)\n",
    "        model = Sequential()\n",
    "        model.add(Lambda(lambda x: x / 255.0))\n",
    "        \n",
    "        #se define una red convolucional como en el paper: \"Playing Atari with Deep Reinforcement Learning\"\n",
    "        model.add(Conv2D(16, 8, strides=(4, 4),activation='relu',input_shape=obs_shape))\n",
    "        model.add(Conv2D(32, 4, strides=(2, 2),activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(n_actions, activation=None))\n",
    "        \n",
    "        # para la red convolucional como la utilizada en el paper de nature de DeepMind descomentar\n",
    "        # y comentar la red anterior\n",
    "        # \"Human-level control through deep reinforcement learning\"\n",
    "        # (https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)\n",
    "        #model = Sequential()\n",
    "        #model.add(Lambda(lambda x: x / 255.0))\n",
    "        #model.add(Conv2D(32, 8, strides=(4, 4),activation='relu',input_shape=obs_shape))\n",
    "        #model.add(Conv2D(64, 4, strides=(2, 2),activation='relu'))\n",
    "        #model.add(Conv2D(64, 3, strides=(1, 1),activation='relu'))\n",
    "        #model.add(Flatten())\n",
    "        #model.add(Dense(512, activation='relu'))\n",
    "        #model.add(Dense(n_actions, activation=None))\n",
    "        \n",
    "        model.compile(loss=\"mse\", optimizer=RMSprop(learning_rate, rho=0.95, epsilon=0.01))\n",
    "        self.model = model\n",
    "     \n",
    "    def update(self, states, q_values, verbose=0):\n",
    "        \"\"\"\n",
    "        Realiza un update de los parámetros de la red neuronal usando un batch de estados y batch de vectores de valores\n",
    "        de la función estado-acción correspondientes\n",
    "        \"\"\"\n",
    "        self.model.fit(states, q_values, verbose=0)\n",
    "    \n",
    "    def predict(self, s):\n",
    "        \"\"\"\n",
    "        Realiza una predicción de la función de valor estado-accion dado el estado\n",
    "        \n",
    "        Argumentos:\n",
    "            s: estado para el cual realizar la predicción\n",
    "            \n",
    "        Retorna:\n",
    "            Un vector con la predicción de la función de valor para todas las accións\n",
    "        \"\"\"\n",
    "        return self.model.predict(self.process_state_for_network(s))\n",
    "    \n",
    "    def process_state_for_network(self, state):\n",
    "        \"\"\"Scale, convert to greyscale and store as float32.\n",
    "        Basically same as process state for memory, but this time\n",
    "        outputs float32 images.\n",
    "        \"\"\"\n",
    "        state = state.astype('float')\n",
    "        return state\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, memory_size=100000, history_length=4):\n",
    "        self.history = np.zeros((84, 84, history_length))\n",
    "        self.history_length = history_length\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Guardar una tupla de estado, accion, recompensa, proximo estado, estado_terminal.\n",
    "        \"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def preprocess_image(self, state):\n",
    "        \"\"\"\n",
    "        Convertir la imagen a blanco y negro y reducirla a 84x84.\n",
    "        \"\"\"\n",
    "        I = Image.fromarray(state, 'RGB')\n",
    "        I = I.convert('L')  # to gray\n",
    "        I = I.resize((84, 84), Image.ANTIALIAS)\n",
    "        I = np.array(I).astype('uint8')\n",
    "        return I\n",
    "    \n",
    "    def push_frame(self, state):\n",
    "        \"\"\"\n",
    "        Pushear el frame a la stack de frames.\n",
    "        \"\"\"\n",
    "        state = self.preprocess_image(state)\n",
    "        self.history[..., 0] = state\n",
    "        self.history = np.roll(self.history, -1, axis=-1)\n",
    "        return self.history.copy()\n",
    "    \n",
    "    def reset_history(self):\n",
    "        \"\"\"\n",
    "        Resetear el stack de frames.\n",
    "        \"\"\"\n",
    "        self.history = np.zeros((84, 84, self.history_length))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, env, estimator, estimator_target, num_steps, discount_factor=1.0, \n",
    "                 exploration_max=1.0, exploration_min=0.05, epsilon_decay=0.99, memory_size=100000, \n",
    "                 batch_size=64, target_update_freq=1000, history_length=4, train_freq=4, burnin=100000,\n",
    "                 save_freq = 50000):\n",
    "        \"\"\"\n",
    "        Algoritmo q-learning/sarsa con experience replay utilizando aproximación de funciones.\n",
    "\n",
    "        Argumentos de inicialización:\n",
    "            env: ambiente de OpenAI.\n",
    "            estimator: función de aproximación de la función de valor estado-acción.\n",
    "            estimator_target: función de aproximación target de la función de valor estado-acción.\n",
    "            num_steps: número de pasos durante los cuales entrenar el estimador.\n",
    "            discount_factor: factor de descuento gama.\n",
    "            exploration_max: probabilidad de tomar una acción aleatoria inicialmente.\n",
    "            exploration_min: mínima probabilidad de tomar una acción aleatoria.\n",
    "            epsilon_decay: luego de cada episodio la probabilidad de una acción aleatoria decae por este factor.\n",
    "            target_update_freq: frecuencia de update de la función de aproximación target.\n",
    "            train_freq: frecuencia de entrenamiento de la función de aproximación de la función de valor.\n",
    "            history_length: cantidad de frames que se mandan a la red neuronal.\n",
    "            burnin: cantidad inicial de pasos exploratorios para tener la memoria inicializada.\n",
    "            save_freq: cada cuántos pasos se guardan los modelos.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.discount_factor = discount_factor\n",
    "        \n",
    "        self.estimator = estimator\n",
    "        self.estimator_target = estimator_target\n",
    "        \n",
    "        self.num_steps = num_steps\n",
    "        \n",
    "        self.exploration_rate = exploration_max\n",
    "        self.exploration_min = exploration_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.train_freq = train_freq\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        self.memory = Memory(memory_size, history_length)\n",
    "        \n",
    "        self.history_length = history_length\n",
    "        \n",
    "        self.burnin = burnin\n",
    "        self.save_freq = save_freq\n",
    "        \n",
    "        \n",
    "    def decay_exploration_rate(self):\n",
    "        # decae la probabilidad de exploración\n",
    "        self.exploration_rate *= self.epsilon_decay\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate)\n",
    "    \n",
    "    def policy_fn(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Política epsilon-greedy basada en la función de aproximación actual y la probabilidad de exploración epsilon.\n",
    "\n",
    "        Retorna:\n",
    "            Un vector con la probabilidad de tomar cada acción.\n",
    "        \"\"\"\n",
    "        A = np.ones(env.action_space.n, dtype=float) * self.exploration_rate / env.action_space.n\n",
    "        q_values = self.estimator.predict(state)\n",
    "        best_action = np.argmax(q_values[0])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        \"\"\"\n",
    "        Realiza una corrida de entrenamiento de la función de aproximación dado un batch de experiencia acumulada.\n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        # samplear de la memoria\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        batch_q_values = np.zeros((self.batch_size, self.env.action_space.n))\n",
    "        batch_states = np.zeros((self.batch_size, 84, 84, self.history_length))\n",
    "        \n",
    "        for i_batch, (state, action, reward, next_state, terminal) in enumerate(batch):\n",
    "            \n",
    "            q_update = reward\n",
    "            if not terminal:\n",
    "                ## el update de q-learning para este ejemplo\n",
    "                q_update = reward + self.discount_factor * np.amax(self.estimator_target.predict(next_state)[0])\n",
    "            \n",
    "            q_values = self.estimator.predict(state)\n",
    "            q_values[0][action] = q_update\n",
    "            batch_q_values[i_batch] = q_values\n",
    "            batch_states[i_batch] = state\n",
    "            i_batch += 1\n",
    "        \n",
    "        # un paso en la dirección del gradiente\n",
    "        self.estimator.update(batch_states, batch_q_values, verbose=0)\n",
    "        \n",
    "    def train(self):    \n",
    "        \"\"\"\n",
    "        Realiza Q-Learning o Sarsa con aproximación de la función de valor estado-acción.\n",
    "        Retorna:\n",
    "            Un objeto de tipo EpisodeStats con dos arrays de numpy para la longitud y recompensa acumulada de cada\n",
    "            episodio respectivamente.\n",
    "        \"\"\"\n",
    "        stats = plotting.EpisodeStats(\n",
    "            episode_lengths=[],\n",
    "            episode_rewards=[])    \n",
    "        \n",
    "        # realizamos self.burnin pasos iniciales de exploración para inicializar la memoria\n",
    "        itr = 0\n",
    "        while itr <= self.burnin:\n",
    "            step = 0\n",
    "            self.memory.reset_history()\n",
    "            state = env.reset()\n",
    "            # guardar en el stack de frames\n",
    "            state = self.memory.push_frame(state)\n",
    "            # hacer un reshape del estado para poder pasarlo por la red neronal\n",
    "            state = np.reshape(state, [1, 84, 84, self.history_length])\n",
    "            # resetear el reward\n",
    "            cum_reward = 0.0\n",
    "            # elegir acción al azar\n",
    "            action = np.random.choice(self.env.action_space.n)\n",
    "            while True:\n",
    "                env.render()\n",
    "                # tomar la acción elegida\n",
    "                next_state, reward, terminal, info = env.step(action)\n",
    "                cum_reward += reward*self.discount_factor**step\n",
    "                # guardar en el stack de frames\n",
    "                next_state = self.memory.push_frame(next_state)\n",
    "                itr += 1; step += 1\n",
    "                # hacer un reshape del estado para poder pasarlo por la red neronal\n",
    "                next_state = np.reshape(next_state, [1, 84, 84, self.history_length])\n",
    "                # elegir la próxima acción al azar\n",
    "                if not terminal:\n",
    "                    next_action = np.random.choice(self.env.action_space.n)\n",
    "                else:\n",
    "                    next_action = None\n",
    "                    self.memory.reset_history()\n",
    "                # almacenar en la memoria\n",
    "                self.memory.remember(state, action, reward, next_state, terminal)\n",
    "                # actualizar estado y acción\n",
    "                state = next_state; action = next_action\n",
    "                if terminal:\n",
    "                    print(\"Iteración de burnin:\" + str(itr) + \n",
    "                          \", Exploración: \" + str(self.exploration_rate) + \n",
    "                          \", Recompensa Acumulada: \" + str(cum_reward))\n",
    "                    break\n",
    "        \n",
    "        # comienza el loop de entrenamiento\n",
    "        itr = 0\n",
    "        step = 0\n",
    "        n_episode = 0 \n",
    "        while itr <=  self.num_steps:\n",
    "            # resetear el stack de frames (nuevo episodio)\n",
    "            self.memory.reset_history()\n",
    "            state = env.reset()\n",
    "            # guardar en el stack de frames\n",
    "            state = self.memory.push_frame(state)\n",
    "            # hacer un reshape del estado para poder pasarlo por la red neronal\n",
    "            state = np.reshape(state, [1, 84, 84, self.history_length])\n",
    "            cum_reward = 0.0    \n",
    "            # elegir la acción\n",
    "            action = np.random.choice(self.env.action_space.n, p=self.policy_fn(state, self.exploration_rate))\n",
    "            while True:\n",
    "                env.render()\n",
    "                # realizar un paso con la acción elegida\n",
    "                next_state, reward, terminal, info = env.step(action)\n",
    "                # acumular la recompensa\n",
    "                cum_reward += reward*self.discount_factor**step\n",
    "                # guardar en el stack de frames\n",
    "                next_state = self.memory.push_frame(next_state)\n",
    "                itr += 1; step += 1\n",
    "                # hacer un reshape del estado para poder pasarlo por la red nueronal\n",
    "                next_state = np.reshape(next_state, [1, 84, 84, self.history_length])\n",
    "                if not terminal:\n",
    "                    # elegir la próxima acción\n",
    "                    next_action = np.random.choice(self.env.action_space.n, p=self.policy_fn(next_state, self.exploration_rate))\n",
    "                else:\n",
    "                    next_action = None\n",
    "                    self.memory.reset_history()\n",
    "                \n",
    "                self.memory.remember(state, action, reward, next_state, terminal)\n",
    "                # actualizar el estado y la acción\n",
    "                state = next_state; action = next_action\n",
    "                if terminal:\n",
    "                    print(\"Episodio: \" + str(n_episode) + \n",
    "                          \", Iteración:\" + str(itr) + \n",
    "                          \", Exploración: \" + str(self.exploration_rate) + \n",
    "                          \", Recompensa Acumulada: \" + str(cum_reward))\n",
    "                    # Actualizar estadísticas\n",
    "                    stats.episode_rewards.append(cum_reward)\n",
    "                    stats.episode_lengths.append(step)\n",
    "                    break        \n",
    "                    \n",
    "                # copiar los pesos de la red siendo entrenada al target\n",
    "                if itr % self.target_update_freq == 0:\n",
    "                    self.estimator_target.model.set_weights(self.estimator.model.get_weights())\n",
    "                # entrenar la red cada train_freq steps\n",
    "                if itr % self.train_freq == 0:\n",
    "                    self.experience_replay()\n",
    "                # guardar la red nueronal\n",
    "                if itr % self.save_freq == 0:\n",
    "                    self.estimator.model.save(\"model_\" + str(itr) + \".h5\")\n",
    "            # decaer la probabilidad de exploración\n",
    "            self.decay_exploration_rate()\n",
    "            n_episode += 1\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(env)\n",
    "estimator_target = Estimator(env)\n",
    "learner = Learner(env, estimator, estimator_target, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = learner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(env)\n",
    "estimator.model = keras.models.load_model(\"model_450000.h5\")\n",
    "estimator_target = Estimator(env)\n",
    "estimator_target.model = keras.models.load_model(\"model_450000.h5\")\n",
    "learner = Learner(env, estimator, estimator_target, 500000, discount_factor=1.0, exploration_max=0.2,  xploration_min=0.05,\n",
    "                  epsilon_decay=0.99, burnin=100000, save_freq = 100000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
